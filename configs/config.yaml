seed: 42
dataset_name: "openai/gsm8k"
model_name: "Qwen/Qwen2.5-0.5B"

# data split ratios
split_ratios:
  sft: 0.8
  rl: 0.1
  val: 0.1

# prompt + CoT length
max_length: 512

# training hyperparams
learning_rate: 2e-5
batch_size: 8
grad_accum_steps: 4
num_epochs: 3
weight_decay: 0.01
warmup_steps: 100

# num workers for data loading
num_workers: 4

# output dirs
sft_output_dir: "check_points/sft_checkpoint"
ppo_output_dir: "check_points/ppo_checkpoint"
grpo_output_dir: "check_points/grpo_checkpoint"
verifier_dir: "check_points/verifier_ckpt_from_answer"

# RL (PPO & GRPO) hyperparameters
rl_batch_size:      8           # how many prompts per RL update
rl_learning_rate:   1e-5        # AdamW LR for PPO
rl_warmup_steps:    200         # warmup for the RL scheduler
ppo_epochs:         4           # inner PPO epochs
ppo_clip_eps:       0.2         # PPO clipping ε
gamma:              0.99        # discount for returns
gae_lambda:         0.95        # GAE λ
kl_coef:            0.1         # β for KL penalty
verifier_coef:      0.5         # α for verifier bonus
max_updates:        1000        # total RL “updates”
save_interval:      50          # how often to checkpoint
gen_max_new_tokens: 128         # how many tokens to sample in RL